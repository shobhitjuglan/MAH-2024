{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9670871,"sourceType":"datasetVersion","datasetId":5909774}],"dockerImageVersionId":30043,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-20T01:04:44.143966Z","iopub.execute_input":"2024-10-20T01:04:44.144324Z","iopub.status.idle":"2024-10-20T01:04:44.154780Z","shell.execute_reply.started":"2024-10-20T01:04:44.144293Z","shell.execute_reply":"2024-10-20T01:04:44.153718Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"/kaggle/input/malware-analysis-hackathon/training_network_trails\n/kaggle/input/malware-analysis-hackathon/evaluation_os_trails\n/kaggle/input/malware-analysis-hackathon/evaluation_network_trails\n/kaggle/input/malware-analysis-hackathon/training_os_trails\n/kaggle/input/malware-analysis-hackathon/evaluation_hardware_trails\n/kaggle/input/malware-analysis-hackathon/training_hardware_trails\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Read the input\n\ntraining_network_file = \"/kaggle/input/malware-analysis-hackathon/training_network_trails\"\ntraining_os_file = \"/kaggle/input/malware-analysis-hackathon/training_os_trails\"\ntraining_hardware_file = \"/kaggle/input/malware-analysis-hackathon/training_hardware_trails\"\ntest_network_file = \"/kaggle/input/malware-analysis-hackathon/evaluation_network_trails\"\ntest_os_file = \"/kaggle/input/malware-analysis-hackathon/evaluation_os_trails\"\ntest_hardware_file = \"/kaggle/input/malware-analysis-hackathon/evaluation_hardware_trails\"\n\n\nnetwork_train = pd.read_csv(training_network_file)\nos_train = pd.read_csv(training_os_file)\nhardware_train = pd.read_csv(training_hardware_file)\n\nnetwork_eval = pd.read_csv(test_network_file)\nos_eval = pd.read_csv(test_os_file)\nhardware_eval = pd.read_csv(test_hardware_file)\n\n# raw_data = pd.read_csv(\"/kaggle/input/malware-detection/Malware dataset.csv\")\n# raw_data.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-10-20T01:04:44.157050Z","iopub.execute_input":"2024-10-20T01:04:44.157428Z","iopub.status.idle":"2024-10-20T01:05:00.990908Z","shell.execute_reply.started":"2024-10-20T01:04:44.157390Z","shell.execute_reply":"2024-10-20T01:05:00.990098Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"network_train = network_train.drop(['Dest_IP', 'Dest_P', 'Src_P'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:05:00.992061Z","iopub.execute_input":"2024-10-20T01:05:00.992325Z","iopub.status.idle":"2024-10-20T01:05:01.000734Z","shell.execute_reply.started":"2024-10-20T01:05:00.992299Z","shell.execute_reply":"2024-10-20T01:05:00.999720Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# data['classification'] = data.classification.map({'benign':0, 'malware':1})\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:05:01.002261Z","iopub.execute_input":"2024-10-20T01:05:01.002662Z","iopub.status.idle":"2024-10-20T01:05:01.012013Z","shell.execute_reply.started":"2024-10-20T01:05:01.002610Z","shell.execute_reply":"2024-10-20T01:05:01.011193Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Preprocessing function\ndef preprocess_data(df):\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    non_numeric_cols = df.select_dtypes(include=['object']).columns\n\n    # Fill missing values in numeric columns with the mean\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n    \n    # Encode non-numeric (categorical) columns\n    label_encoders = {}\n    for col in non_numeric_cols:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col].astype(str))\n        label_encoders[col] = le  # Store encoders for reverse mapping if needed\n\n    return df, label_encoders","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:17:58.977638Z","iopub.execute_input":"2024-10-20T01:17:58.978021Z","iopub.status.idle":"2024-10-20T01:17:58.987969Z","shell.execute_reply.started":"2024-10-20T01:17:58.977990Z","shell.execute_reply":"2024-10-20T01:17:58.986965Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Neural Network model definition\ndef build_model(input_dim, output_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n    model.add(Dropout(0.3))  # Add dropout to avoid overfitting\n\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.2))\n\n    model.add(Dense(output_dim, activation='softmax'))  # Output layer for multi-class classification\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-20T02:01:55.707164Z","iopub.execute_input":"2024-10-20T02:01:55.707529Z","iopub.status.idle":"2024-10-20T02:01:55.716569Z","shell.execute_reply.started":"2024-10-20T02:01:55.707496Z","shell.execute_reply":"2024-10-20T02:01:55.715479Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Train and Predict Function\ndef train_and_predict(df_train, df_eval, target_column, eval_ids, csv_filename):\n    # Preprocess the data\n    df_train, label_encoders = preprocess_data(df_train)\n    df_eval, _ = preprocess_data(df_eval)\n    \n    # Separate features and target for training\n    X_train = df_train.drop(columns=[target_column])\n    y_train = df_train[target_column]\n    X_eval = df_eval.drop(columns=[target_column], errors='ignore')  # Evaluation data has no target\n    \n    # Scale the data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_eval = scaler.transform(X_eval)\n    \n    # Build and train the model\n    model = build_model(input_dim=X_train.shape[1], output_dim=9)  # 9 classes\n    model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n    \n    # Predict on the evaluation set\n    y_eval_pred = model.predict(X_eval)\n    y_eval_pred_classes = y_eval_pred.argmax(axis=1)  # Get class with max probability\n    \n    # Reverse encoding for the 'goal' column\n#     le_goal = label_encoders.get(target_column)\n#     y_eval_pred_labels = le_goal.inverse_transform()\n    \n    # Create DataFrame for the output\n    output_df = pd.DataFrame({\n        'Id': eval_ids,\n        'Goal': y_eval_pred_classes\n    })\n    \n    # Save to CSV\n    output_df.to_csv(csv_filename, index=False)\n    print(f\"Predictions saved to {csv_filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-20T02:02:01.136369Z","iopub.execute_input":"2024-10-20T02:02:01.136717Z","iopub.status.idle":"2024-10-20T02:02:01.147266Z","shell.execute_reply.started":"2024-10-20T02:02:01.136685Z","shell.execute_reply":"2024-10-20T02:02:01.146390Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Combine all predictions and save to CSV\ndef save_predictions(eval_ids, y_eval_pred, type_):\n    output_df = pd.DataFrame({\n        'Id': eval_ids,  # 'Id' column from the evaluation dataset\n        'Goal': y_eval_pred  # Predicted 'Goal' labels\n    })\n\n    # Save the predictions to a CSV file\n    output_filename = f\"{type_}.csv\"\n    output_df.to_csv(output_filename, index=False)\n    print(f\"Predictions saved to {output_filename}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T02:02:04.199314Z","iopub.execute_input":"2024-10-20T02:02:04.199671Z","iopub.status.idle":"2024-10-20T02:02:04.205678Z","shell.execute_reply.started":"2024-10-20T02:02:04.199619Z","shell.execute_reply":"2024-10-20T02:02:04.204571Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Combine CSVs Function\ndef combine_csvs(csv_files, output_filename):\n    dfs = [pd.read_csv(csv) for csv in csv_files]\n    combined_df = pd.concat(dfs, ignore_index=True)\n    combined_df.to_csv(output_filename, index=False)\n    print(f\"Combined CSV saved to {output_filename}\")\n\n# Create eval_ids for the hardware dataset\neval_ids_h = pd.Series(range(1, len(hardware_eval) + 1))\n\n# Create eval_ids for the network dataset\neval_ids_n = pd.Series(range(len(hardware_eval) + 1, len(hardware_eval) + len(network_eval) + 1))\n\n# Create eval_ids for the OS dataset\neval_ids_o = pd.Series(range(len(hardware_eval) + len(network_eval) + 1, len(hardware_eval) + len(network_eval) + len(os_eval) + 1))\n\n\n\ntrain_and_predict(hardware_train, hardware_eval, 'goal', eval_ids_h, 'hardware_predictions.csv')\ntrain_and_predict(network_train, network_eval, 'goal', eval_ids_n, 'network_predictions.csv')\ntrain_and_predict(os_train, os_eval, 'goal', eval_ids_o, 'os_predictions.csv')\n\n# Combine all CSVs into one final CSV\ncombine_csvs(['hardware_predictions.csv', 'network_predictions.csv', 'os_predictions.csv'], 'final_predictions.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:38:48.578417Z","iopub.execute_input":"2024-10-20T01:38:48.578775Z","iopub.status.idle":"2024-10-20T01:53:25.792958Z","shell.execute_reply.started":"2024-10-20T01:38:48.578744Z","shell.execute_reply":"2024-10-20T01:53:25.792026Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Epoch 1/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0370 - accuracy: 0.9898 - val_loss: 0.0030 - val_accuracy: 0.9991\nEpoch 2/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0029 - val_accuracy: 0.9992\nEpoch 3/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0021 - val_accuracy: 0.9995\nEpoch 4/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9991\nEpoch 5/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0015 - val_accuracy: 0.9997\nEpoch 6/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0012 - val_accuracy: 0.9997\nEpoch 7/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0041 - val_accuracy: 0.9995\nEpoch 8/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 6.6446e-04 - val_accuracy: 0.9998\nEpoch 9/10\n10048/10048 [==============================] - 21s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0021 - val_accuracy: 0.9995\nEpoch 10/10\n10048/10048 [==============================] - 22s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0042 - val_accuracy: 0.9996\nPredictions saved to hardware_predictions.csv\nEpoch 1/10\n123/123 [==============================] - 0s 3ms/step - loss: 1.1616 - accuracy: 0.6317 - val_loss: 0.2971 - val_accuracy: 0.9096\nEpoch 2/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.3469 - accuracy: 0.8945 - val_loss: 0.0986 - val_accuracy: 0.9766\nEpoch 3/10\n123/123 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9569 - val_loss: 0.0483 - val_accuracy: 0.9898\nEpoch 4/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9739 - val_loss: 0.0397 - val_accuracy: 0.9893\nEpoch 5/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9831 - val_loss: 0.0265 - val_accuracy: 0.9903\nEpoch 6/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9882 - val_loss: 0.0208 - val_accuracy: 0.9924\nEpoch 7/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9909 - val_loss: 0.0114 - val_accuracy: 0.9964\nEpoch 8/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9929 - val_loss: 0.0085 - val_accuracy: 0.9975\nEpoch 9/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9940 - val_loss: 0.0099 - val_accuracy: 0.9980\nEpoch 10/10\n123/123 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9953 - val_loss: 0.0067 - val_accuracy: 0.9985\nPredictions saved to network_predictions.csv\nEpoch 1/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0476 - val_accuracy: 0.9988\nEpoch 2/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 5.7797e-04 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 0.9997\nEpoch 3/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 5.3587e-04 - accuracy: 0.9999 - val_loss: 0.0029 - val_accuracy: 0.9998\nEpoch 4/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 4.9506e-04 - accuracy: 0.9999 - val_loss: 0.0020 - val_accuracy: 0.9998\nEpoch 5/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 7.3497e-04 - accuracy: 0.9999 - val_loss: 0.0015 - val_accuracy: 0.9998\nEpoch 6/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 5.7468e-04 - accuracy: 0.9999 - val_loss: 0.0025 - val_accuracy: 0.9998\nEpoch 7/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 5.5205e-04 - accuracy: 0.9999 - val_loss: 5.2199e-04 - val_accuracy: 1.0000\nEpoch 8/10\n28877/28877 [==============================] - 63s 2ms/step - loss: 4.6258e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9997\nEpoch 9/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 6.0521e-04 - accuracy: 0.9999 - val_loss: 0.0111 - val_accuracy: 0.9997\nEpoch 10/10\n28877/28877 [==============================] - 62s 2ms/step - loss: 5.1766e-04 - accuracy: 0.9999 - val_loss: 0.0093 - val_accuracy: 0.9997\nPredictions saved to os_predictions.csv\nCombined CSV saved to final_predictions.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"goal_mapping = {\n    0:'backdoor',\n    1:'banker',\n    2:'cryptominer',\n    3:'deceptor',\n    4:'downloader',\n    5:'normal',\n    6:'pua',\n    7:'ransomware',\n    8:'spyware',\n}\n\n# Concatenate the datasets into a single DataFrame\ncombined_df = pd.read_csv(\"final_predictions.csv\")\ncombined_df['Goal'] = combined_df['Goal'].map(goal_mapping)\ncombined_df.to_csv('combined_predictons_with_goals.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:54:21.245587Z","iopub.execute_input":"2024-10-20T01:54:21.245952Z","iopub.status.idle":"2024-10-20T01:54:22.626157Z","shell.execute_reply.started":"2024-10-20T01:54:21.245922Z","shell.execute_reply":"2024-10-20T01:54:22.625444Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# corrMatrix = data.corr()\n# sns.heatmap(corrMatrix, annot=True)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T01:05:01.103158Z","iopub.status.idle":"2024-10-20T01:05:01.103624Z"},"trusted":true},"execution_count":null,"outputs":[]}]}